{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d65cfe76-58b5-4e7e-811f-fd20a3e939f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from torchvision.datasets import CIFAR100\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "709df034-d034-4266-bbda-b7d67d9cb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Carregando CIFAR-100 (train split)...\n",
      "üìä Agrupando por classe...\n",
      "üîÄ Embaralhando e separando...\n",
      "üíæ Salvando treino CIFAR-100...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\captchaClassification\\lib\\site-packages\\PIL\\ImageFile.py:536\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    537\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 121\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# ===============\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Execu√ß√£o\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# ===============\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 98\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     95\u001b[0m     val_indices\u001b[38;5;241m.\u001b[39mextend(idx_list[TRAIN_PER_CLASS:TRAIN_PER_CLASS \u001b[38;5;241m+\u001b[39m VAL_PER_CLASS])\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíæ Salvando treino CIFAR-100...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mprocess_and_save_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_DIR_CIFAR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTARGET_SIZE_CIFAR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m imagens de treino salvas.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müíæ Salvando valida√ß√£o CIFAR-100...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m, in \u001b[0;36mprocess_and_save_split\u001b[1;34m(indices, dataset, save_dir, target_size)\u001b[0m\n\u001b[0;32m     33\u001b[0m img_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, img_filename)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mimg_resized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\captchaClassification\\lib\\site-packages\\PIL\\Image.py:2439\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2436\u001b[0m         fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw+b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2439\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\captchaClassification\\lib\\site-packages\\PIL\\PngImagePlugin.py:1402\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1398\u001b[0m     im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1399\u001b[0m         im, fp, chunk, rawmode, default_image, append_images\n\u001b[0;32m   1400\u001b[0m     )\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1405\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\captchaClassification\\lib\\site-packages\\PIL\\ImageFile.py:540\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    538\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 540\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    542\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\captchaClassification\\lib\\site-packages\\PIL\\ImageFile.py:559\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 559\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    560\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    561\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# =====================\n",
    "# Configura√ß√µes Gerais\n",
    "# =====================\n",
    "RANDOM_SEED = 42\n",
    "TARGET_SIZE_CIFAR = (256, 256)\n",
    "TRAIN_PER_CLASS = 400\n",
    "VAL_PER_CLASS = 80\n",
    "\n",
    "# CIFAR-100\n",
    "BASE_INPUT_DIR = \"data\"\n",
    "BASE_OUTPUT_DIR = \"datasets/cifar100\"\n",
    "TRAIN_DIR_CIFAR = os.path.join(BASE_OUTPUT_DIR, \"train\")\n",
    "VAL_DIR_CIFAR = os.path.join(BASE_OUTPUT_DIR, \"val\")\n",
    "\n",
    "#https://www.kaggle.com/datasets/mikhailma/test-dataset\n",
    "\n",
    "# Google reCAPTCHA v2\n",
    "RECAPTCHA_SOURCE_DIR = 'Google_Recaptchav2/images'\n",
    "RECAPTCHA_TRAIN_DIR = 'datasets/recaptcha/train'\n",
    "RECAPTCHA_VAL_DIR = 'datasets/recaptcha/val'\n",
    "\n",
    "# =================================\n",
    "# CIFAR-100: Salvamento de imagens\n",
    "# =================================\n",
    "def process_and_save_split(indices, dataset, save_dir, target_size):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for idx in indices:\n",
    "        img, label = dataset[idx]\n",
    "        class_name = dataset.classes[label]\n",
    "        img_resized = img.resize(target_size, Image.BILINEAR)\n",
    "        class_dir = os.path.join(save_dir, class_name)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        img_filename = f\"{idx:05d}.png\"\n",
    "        img_path = os.path.join(class_dir, img_filename)\n",
    "        img_resized.save(img_path)\n",
    "\n",
    "# =======================================\n",
    "# Google reCAPTCHA v2: C√≥pia e divis√£o\n",
    "# =======================================\n",
    "\n",
    "def process_recaptcha_dataset():\n",
    "    if not os.path.exists(RECAPTCHA_SOURCE_DIR):\n",
    "        print(f\"‚ùå O diret√≥rio '{RECAPTCHA_SOURCE_DIR}' n√£o foi encontrado.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"üìÅ Diret√≥rio encontrado: '{RECAPTCHA_SOURCE_DIR}'\")\n",
    "\n",
    "    os.makedirs(RECAPTCHA_TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(RECAPTCHA_VAL_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"üîç Lendo classes do reCAPTCHA v2...\")\n",
    "    classes = os.listdir(dataset_dir)\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_path = os.path.join(dataset_dir, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            os.makedirs(os.path.join(RECAPTCHA_TRAIN_DIR, class_name), exist_ok=True)\n",
    "            os.makedirs(os.path.join(RECAPTCHA_VAL_DIR, class_name), exist_ok=True)\n",
    "\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            random.shuffle(images)\n",
    "\n",
    "            val_size = int(len(images) * 0.2)\n",
    "            val_images = images[:val_size]\n",
    "            train_images = images[val_size:]\n",
    "\n",
    "            for idx, image in enumerate(train_images, 1):\n",
    "                new_name = f\"{idx}_{image}\"\n",
    "                shutil.copy(os.path.join(class_path, image), os.path.join(RECAPTCHA_TRAIN_DIR, class_name, new_name))\n",
    "\n",
    "            for idx, image in enumerate(val_images, 1):\n",
    "                new_name = f\"{idx}_{image}\"\n",
    "                shutil.copy(os.path.join(class_path, image), os.path.join(RECAPTCHA_VAL_DIR, class_name, new_name))\n",
    "\n",
    "    print(\"‚úÖ reCAPTCHA v2: divis√£o 80/20 conclu√≠da!\")\n",
    "    \n",
    "# ===============\n",
    "# Pipeline geral\n",
    "# ===============\n",
    "def main():\n",
    "    print(\"üîç Carregando CIFAR-100 (train split)...\")\n",
    "    dataset = CIFAR100(root=BASE_INPUT_DIR, train=True, download=True)\n",
    "\n",
    "    print(\"üìä Agrupando por classe...\")\n",
    "    class_to_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.targets):\n",
    "        class_to_indices[label].append(idx)\n",
    "\n",
    "    print(\"üîÄ Embaralhando e separando...\")\n",
    "    random.seed(RANDOM_SEED)\n",
    "    train_indices, val_indices = [], []\n",
    "    for label, idx_list in class_to_indices.items():\n",
    "        random.shuffle(idx_list)\n",
    "        train_indices.extend(idx_list[:TRAIN_PER_CLASS])\n",
    "        val_indices.extend(idx_list[TRAIN_PER_CLASS:TRAIN_PER_CLASS + VAL_PER_CLASS])\n",
    "\n",
    "    print(\"üíæ Salvando treino CIFAR-100...\")\n",
    "    process_and_save_split(train_indices, dataset, TRAIN_DIR_CIFAR, TARGET_SIZE_CIFAR)\n",
    "    print(f\"‚úÖ {len(train_indices)} imagens de treino salvas.\")\n",
    "\n",
    "    print(\"üíæ Salvando valida√ß√£o CIFAR-100...\")\n",
    "    process_and_save_split(val_indices, dataset, VAL_DIR_CIFAR, TARGET_SIZE_CIFAR)\n",
    "    print(f\"‚úÖ {len(val_indices)} imagens de valida√ß√£o salvas.\")\n",
    "\n",
    "    print(\"üß† Processando reCAPTCHA v2...\")\n",
    "    process_recaptcha_dataset()\n",
    "\n",
    "    print(\"üìÑ Salvando log...\")\n",
    "    os.makedirs(\"outputs\", exist_ok=True)\n",
    "    with open(\"outputs/logs.txt\", \"w\") as f:\n",
    "        f.write(f\"CIFAR-100 Treino: {len(train_indices)}\\n\")\n",
    "        f.write(f\"CIFAR-100 Valida√ß√£o: {len(val_indices)}\\n\")\n",
    "        f.write(f\"Resolu√ß√£o CIFAR: {TARGET_SIZE_CIFAR[0]}x{TARGET_SIZE_CIFAR[1]}\\n\")\n",
    "\n",
    "    print(\"üèÅ Tudo finalizado com sucesso!\")\n",
    "\n",
    "# ===============\n",
    "# Execu√ß√£o\n",
    "# ===============\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617192b8-f076-4d0b-9503-dcbf8d0108a1",
   "metadata": {},
   "source": [
    "# Cria√ß√£o GRv2P (Google Recaptcha v2 p√≥s-holdout) para treinamento dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82acb52-3cd9-4871-b7d0-611dd60a3495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diret√≥rio de origem 'datasets/Google_Recaptchav2/images' foi encontrado.\n",
      "Diret√≥rios de treino: GRv2P/train\n",
      "Diret√≥rios de valida√ß√£o: GRv2P/val\n",
      "Distribui√ß√£o 80-20 conclu√≠da com c√≥pias das imagens e n√∫meros crescentes!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Caminhos de origem e destino\n",
    "dataset_dir = 'datasets/Google_Recaptchav2/images'\n",
    "train_dir = 'datasets/GRv2P/train'\n",
    "val_dir = 'datasets/GRv2P/val'\n",
    "\n",
    "# Verificar se o diret√≥rio de origem existe\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(f\"O diret√≥rio de origem '{dataset_dir}' n√£o existe.\")\n",
    "else:\n",
    "    print(f\"O diret√≥rio de origem '{dataset_dir}' foi encontrado.\")\n",
    "\n",
    "# Cria√ß√£o das pastas de treino e valida√ß√£o, se n√£o existirem\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Verificar se as pastas de destino foram criadas corretamente\n",
    "print(f\"Diret√≥rios de treino: {train_dir}\")\n",
    "print(f\"Diret√≥rios de valida√ß√£o: {val_dir}\")\n",
    "\n",
    "# Listar as classes\n",
    "classes = os.listdir(dataset_dir)\n",
    "\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(dataset_dir, class_name)\n",
    "\n",
    "    # Verificar se √© uma pasta e contornar se for\n",
    "    if os.path.isdir(class_path):\n",
    "        # Criar diret√≥rios de classe dentro de train e val\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
    "\n",
    "        # Listar todas as imagens na classe\n",
    "        images = os.listdir(class_path)\n",
    "        images = [img for img in images if img.endswith(('.jpg', '.jpeg', '.png'))]  # Ajuste os formatos se necess√°rio\n",
    "\n",
    "        # Embaralhar as imagens aleatoriamente\n",
    "        random.shuffle(images)\n",
    "\n",
    "        # Determinar o tamanho para valida√ß√£o (15%)\n",
    "        val_size = int(len(images) * 0.15)\n",
    "\n",
    "        # Separar imagens para val e train\n",
    "        val_images = images[:val_size]\n",
    "        train_images = images[val_size:]\n",
    "\n",
    "        # Copiar as imagens para a pasta correta (train e val) com n√∫mero crescente no nome\n",
    "        for idx, image in enumerate(train_images, 1):\n",
    "            # Criar novo nome com n√∫mero crescente\n",
    "            new_image_name = f\"{idx}_{image}\"\n",
    "            shutil.copy(os.path.join(class_path, image), os.path.join(train_dir, class_name, new_image_name))\n",
    "\n",
    "        for idx, image in enumerate(val_images, 1):\n",
    "            # Criar novo nome com n√∫mero crescente\n",
    "            new_image_name = f\"{idx}_{image}\"\n",
    "            shutil.copy(os.path.join(class_path, image), os.path.join(val_dir, class_name, new_image_name))\n",
    "\n",
    "print(\"Distribui√ß√£o 80-20 conclu√≠da com c√≥pias das imagens e n√∫meros crescentes!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8f826-9f7e-4546-81f8-6a63833bb79f",
   "metadata": {},
   "source": [
    "# Cria√ß√£o/c√≥pia do dataset GRv2P ap√≥s aplicar ruido Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c59be6-ac3d-4ce0-a0ef-33d97911392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aplicando ru√≠do e distor√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1754/1754 [01:04<00:00, 27.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# Caminhos\n",
    "val_dir = 'datasets/GRv2P/val'\n",
    "val_aug_dir = 'datasets/GRv2P/val_aug'\n",
    "\n",
    "# Cria diret√≥rio de sa√≠da base\n",
    "os.makedirs(val_aug_dir, exist_ok=True)\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, std=15):\n",
    "    noise = np.random.normal(mean, std, image.shape).astype(np.float32)\n",
    "    noisy = image.astype(np.float32) + noise\n",
    "    noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def wave_distortion(image, amplitude=1, wavelength=1.6):\n",
    "    rows, cols = image.shape[:2]\n",
    "    map_x = np.zeros((rows, cols), dtype=np.float32)\n",
    "    map_y = np.zeros((rows, cols), dtype=np.float32)\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            dx = amplitude * np.sin(2 * np.pi * i / wavelength)\n",
    "            dy = amplitude * np.cos(2 * np.pi * j / wavelength)\n",
    "            map_x[i, j] = j + dx\n",
    "            map_y[i, j] = i + dy\n",
    "\n",
    "    return cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "# Percorre todas as imagens nas subpastas\n",
    "image_paths = glob(os.path.join(val_dir, '**', '*.*'), recursive=True)\n",
    "\n",
    "for img_path in tqdm(image_paths, desc=\"Aplicando ru√≠do e distor√ß√£o\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    # Aplica filtros\n",
    "    noisy_img = add_gaussian_noise(img)\n",
    "    distorted_img = wave_distortion(noisy_img)\n",
    "\n",
    "    # Caminho relativo da imagem para manter a estrutura de pastas\n",
    "    rel_path = os.path.relpath(img_path, val_dir)\n",
    "    save_path = os.path.join(val_aug_dir, rel_path)\n",
    "\n",
    "    # Cria subdiret√≥rio se necess√°rio\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    # Salva imagem\n",
    "    cv2.imwrite(save_path, distorted_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bcc5fb-a97f-443f-8eea-5ac415a31351",
   "metadata": {},
   "source": [
    "# Cria√ß√£o/c√≥pia do dataset CIFAR-100 ap√≥s aplicar ruido Gaussiano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a39495b-c8c4-4ff1-ad3d-82cb1d55a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aplicando ru√≠do e distor√ß√£o: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8000/8000 [19:18<00:00,  6.90it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Diret√≥rio onde est√£o as imagens com ru√≠do e distor√ß√£o\n",
    "val_dir = 'datasets/cifar100/val'       # imagens originais\n",
    "val_aug_dir = 'datasets/cifar100/val_aug'  # onde vamos salvar as distorcidas\n",
    "\n",
    "# Cria diret√≥rio de sa√≠da base\n",
    "os.makedirs(val_aug_dir, exist_ok=True)\n",
    "\n",
    "# Filtros\n",
    "def add_gaussian_noise(image, mean=0, std=15):\n",
    "    noise = np.random.normal(mean, std, image.shape).astype(np.float32)\n",
    "    noisy = image.astype(np.float32) + noise\n",
    "    noisy = np.clip(noisy, 0, 255).astype(np.uint8)\n",
    "    return noisy\n",
    "\n",
    "def wave_distortion(image, amplitude=1, wavelength=1.6):\n",
    "    rows, cols = image.shape[:2]\n",
    "    map_x = np.zeros((rows, cols), dtype=np.float32)\n",
    "    map_y = np.zeros((rows, cols), dtype=np.float32)\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            dx = amplitude * np.sin(2 * np.pi * i / wavelength)\n",
    "            dy = amplitude * np.cos(2 * np.pi * j / wavelength)\n",
    "            map_x[i, j] = j + dx\n",
    "            map_y[i, j] = i + dy\n",
    "\n",
    "    return cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "# Aplica ru√≠do e distor√ß√£o\n",
    "image_paths = glob(os.path.join(val_dir, '**', '*.*'), recursive=True)\n",
    "for img_path in tqdm(image_paths, desc=\"Aplicando ru√≠do e distor√ß√£o\"):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        continue\n",
    "\n",
    "    noisy_img = add_gaussian_noise(img)\n",
    "    distorted_img = wave_distortion(noisy_img)\n",
    "\n",
    "    # Caminho relativo e novo nome com \"aug\"\n",
    "    rel_path = os.path.relpath(img_path, val_dir)\n",
    "    base, ext = os.path.splitext(rel_path)\n",
    "    new_name = base + \"_aug\" + ext\n",
    "    save_path = os.path.join(val_aug_dir, new_name)\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    cv2.imwrite(save_path, distorted_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
